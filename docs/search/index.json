[{"content":" Economic forecasts have proven to be as unreliable as predictions on infection rates. Part of the difficulty lies in economics being downstream from the development of the pandemic. Higher infection rates leads to lower mobility which leads to lower economic activity. In these cases, complexity in modeling doesn\u0026rsquo;t necessarily help. My idea is to circumnavigate the pandemic predictions in order to make 1-week-ahead forecasts of movement patterns with a novel data source and a simple model. The underlying theory has been advocated for as \u0026ldquo;narrative economics\u0026rdquo; by Robert Shiller and George Akerlof for some time now. But it has featured as a theme in many works without being mentioned by name, such as in Galbraith\u0026rsquo;s \u0026ldquo;The Great Crash of 1929\u0026rdquo; from 1955. Tracing its origins takes us back as far as the 1930\u0026rsquo;s when Keynes coined the phrase \u0026ldquo;animal spirits\u0026rdquo;, meant to capture a characteristic of human behavior beyond what was imagined in the classical models of economics. The underlying assumption is that economic outcomes, to some extent, is a function of the stories and ideas people spread. When these stories reach a wide and receptive audience they turn economic behavior into heard behavior.\nSo what would be the utility of predicting changes in movement patterns? A sharp drop in movement can be categorized as a black swan event for affected parties, whether they are retail stores, public transport companies or government agencies. For public transport, even a 1-week-ahead forecast of a sharp drop or increase in commuting could be useful (I would think).\nData To capture the narrative component I turned to tried and true Google Trends. It\u0026rsquo;s convenient, free and there\u0026rsquo;s a package for R called gtrendsR which does the API call for you so you don\u0026rsquo;t have to cURL it. The data however is a bit wonky, in the sense that Google provides the amount of hits as an index which is calculated in a black box of unpredictable magic, as noted by Shiller in his 2019 book Narrative Economics (but not in those words).\nFor data on movement patterns I used Google Mobility Report, one of the most interesting publicly available data sets on the internet. It was launched in the infancy of the Covid-19 pandemic to track changes in movement patterns all over the globe. It calculates changes from the same days baseline categorized by country, sub region and type of location/activity (retail and recreation, parks, homes etc).\nModel One of the problems with a simple linear regression model is that time series data are likely to be autocorrelated which will show up in the residuals and violate all kinds of fancy OLS assumptions. One way of resolving this is to model the residuals as an ARIMA-process. I ended up with a (1,1,0) process here. So the model used will be a linear regression with ARIMA errors. Sometimes referred to as ARIMAX, where the X denotes an external regressor.\nIn the model, narratives are spread at time t and have an effect on economic behavior at time t+1. Having input variables lagged at t+1 allows us to use external regressors as fresh input for 1-step-ahead prediction in an ARIMAX model. My idea was to have 1 variable which is a proxy for to what extent people spread information about a virus. I ended up with covid and virus. In this case, as the word spreads about Covid-19, people go online to search for information which registers in the index from Google Trends. This variable should be negatively correlated with movement patterns. And then another variable that captures the behavioral change. For this variable my query in Google Trends was sn채llt책get and sj which are the main operators of long distance trains in Sweden. The assumption here is that people on average go online and search for train tickets 1 week ahead of departure. This variable should be positively correlated with movement patterns.\nR code library(tidyverse) library(gtrendsR) # for Google Trends API calls. library(fable) # tidyverse compatible replacement of the forecast package. library(feasts) library(tsibble) library(lubridate) # to help with some weekly time series strangeness. These were the inputs I went with for the Google Trends API call.\nquery \u0026lt;- c(\u0026quot;covid\u0026quot;, \u0026quot;virus\u0026quot;) query2 \u0026lt;- c(\u0026quot;sn채llt책get\u0026quot;, \u0026quot;sj\u0026quot;) date \u0026lt;- c(\u0026quot;2020-03-01 2021-01-15\u0026quot;) I wrote a function which loops along the query vectors and outputs the mean of hits (index of times searched) in a data frame. This way it\u0026rsquo;s easy to experiment with different search queries and do some explorative data analysis.\nsearch_se \u0026lt;- data.frame() search_se_mean \u0026lt;- function(query) { for(i in seq_along(search)) { print(search[i]) search_se \u0026lt;- rbind(covid_se, gtrends(keyword = search[i], geo = \u0026quot;SE\u0026quot;, time = date)[[1]]) } search_se_mean \u0026lt;- search_se %\u0026gt;% mutate(week = yearweek(date, week_start = 1)) %\u0026gt;% group_by(week) %\u0026gt;% summarise(hits = mean(hits)) } gtrends_nar \u0026lt;- search_se_mean(query) gtrends_beh \u0026lt;- search_se_mean(query2) gtrends_se \u0026lt;- gtrends_nar %\u0026gt;% full_join(gtrends_beh, by = \u0026quot;week\u0026quot;, suffix = c(\u0026quot;_nar\u0026quot;, \u0026quot;_beh\u0026quot;)) Let\u0026rsquo;s plot them together.\n \nHits for train travel drops sharply, as one would expect, and then rebounds over the summer. Hits for the virus jumps up but starts dropping surprisingly fast. Lower levels over the summer is in line with lower spread. Come autumn and the index jumps up again. This inverse relationship between narrative and behavioral predictors make intuitive sense and looks promising.\nNext we\u0026rsquo;ll see how the predictors match with what we are trying to predict: movement patterns. For the movement data, go to https://www.google.com/covid19/mobility/ and download a CSV-file. I went with global data and did the filtering in R. Once it\u0026rsquo;s loaded, the following code will filter for the target country with country_region_code. I filtered for sub_region_1 = \u0026quot;\u0026quot;  in order to capture data for all of Sweden. The data is then transformed into weeks. Note that I used the variable retail_and_recreation_percent_change_from_baseline.\ngmr_se \u0026lt;- gmr %\u0026gt;% mutate(date = as_date(date)) %\u0026gt;% filter(country_region_code == \u0026quot;SE\u0026quot;, sub_region_1 == \u0026quot;\u0026quot;, date \u0026gt;= \u0026quot;2020-03-01\u0026quot; \u0026amp; date \u0026lt;= \u0026quot;2021-01-17\u0026quot;) %\u0026gt;% select(date, retail_and_recreation_percent_change_from_baseline) %\u0026gt;% mutate(week = yearweek(date, week_start = 1)) %\u0026gt;% group_by(week) %\u0026gt;% summarise(across(everything(), list(mean))) %\u0026gt;% rename( \u0026quot;retail\u0026quot; = retail_and_recreation_percent_change_from_baseline_1) %\u0026gt;% select(week, retail) df_se \u0026lt;- gtrends_se %\u0026gt;% # join data sets together left_join(gmr_se) df_se$retail \u0026lt;- df_se$retail + 100 # for potential differencing and log transformations I inspected the relationship between y, x1 and x2 while running regressions with and without lag at the same time. Inspecting the plots we see that there is a linear relationship between the variables. The narrative (virus search) variable does well with 1 lag, while the behavioral (train ticket search) does better without a lag. But this will depend a lot on the data you get from your queries, and on the reliability of Google\u0026rsquo;s black box of magic. So we will stick with the theory in order to be able to predict 1-step-ahead. Lagged variables it is.\n \nPreparing the data for model fitting.\nval_weeks \u0026lt;- 15 #15 weeks for validating the models df_se_index \u0026lt;- df_se %\u0026gt;% mutate(index = seq_along(1:nrow(.)), type = if_else(index \u0026gt; max(index) - val_weeks, \u0026quot;validation\u0026quot;, \u0026quot;training\u0026quot;), hits_nar_lag = lag(hits_nar), hits_beh_lag = lag(hits_beh)) tsibble_se \u0026lt;- as_tsibble(df_se_index, index = index) # as time series tibble tsibble_se_train \u0026lt;- tsibble_se %\u0026gt;% filter(type == \u0026quot;training\u0026quot;) tsibble_se_val \u0026lt;- tsibble_se %\u0026gt;% filter(type == \u0026quot;validation\u0026quot;) Let\u0026rsquo;s compute both the TSLM and the ARIMAX (1,1,0) model to see if it makes sense to model the residuals as an ARIMA-process. The equation for the ARIMAX model is   where   is the ARIMA error term. Prime notations for differencing are missing here.\nfit_tslm \u0026lt;- tsibble_se_train %\u0026gt;% # fit the model on the training data model(TSLM(retail ~ hits_nar_lag + hits_beh_lag)) fc_tslm \u0026lt;- fit_tslm %\u0026gt;% # forecast with the validation data forecast(new_data = tsibble_se_val) fit_arimax \u0026lt;- tsibble_se_train %\u0026gt;% model(ARIMA(retail ~ hits_nar_lag + hits_beh_lag + pdq(1, 1, 0))) fc_arimax \u0026lt;- fit_arimax %\u0026gt;% forecast(new_data = tsibble_se_val) rmse_tslm \u0026lt;- round(accuracy(fit_tslm)[, 4], digits = 2) # extract RMSE rmse_arimax \u0026lt;- round(accuracy(fit_arimax)[, 4], digits = 2) # extract RMSE fit_tslm %\u0026gt;% gg_tsresiduals() The patterns at index \u0026gt; 18 or so don\u0026rsquo;t look like a white noise-process to me. This shows up in the ACF plot as well, even though the spikes aren\u0026rsquo;t significant. This will depend a lot on your data, so I will try fitting the ARIMAX model.  TSLM residuals \nLet\u0026rsquo;s evaluate the same set of plots for ARIMAX. This looks more like a stationary white noise process. No significant spikes.\n ARIMAX residuals \nFor plotting models and forecasts produced with Fable, I went with this:\ntslm_plot \u0026lt;- tsibble_se %\u0026gt;% mutate(color = if_else(type == \u0026quot;training\u0026quot;, \u0026quot;#7e828c\u0026quot;, \u0026quot;#7e828c\u0026quot;)) %\u0026gt;% ggplot(aes(x = index, y = retail)) + geom_line() + autolayer(fc_tslm, alpha = 0.5, color = \u0026quot;#aa332c\u0026quot;) + geom_line(aes(color = color), alpha = 0.8) + geom_line(aes(y = .fitted, color = \u0026quot;#aa332c\u0026quot;), data = augment(fit_tslm)) + labs( title = \u0026quot;TSLM model\u0026quot;, subtitle = paste0(\u0026quot;RMSE = \u0026quot;, rmse_tslm)) + scale_color_identity() arimax_plot \u0026lt;- tsibble_se %\u0026gt;% mutate(color = if_else(type == \u0026quot;training\u0026quot;, \u0026quot;#7e828c\u0026quot;, \u0026quot;#7e828c\u0026quot;)) %\u0026gt;% ggplot(aes(x = index, y = retail)) + geom_line() + autolayer(fc_arimax, alpha = 0.5, color = \u0026quot;#aa332c\u0026quot;) + geom_line(aes(color = color), alpha = 0.8) + geom_line(aes(y = .fitted, color = \u0026quot;#aa332c\u0026quot;), data = augment(fit_arimax)) + labs( title = \u0026quot;ARIMAX model\u0026quot;, subtitle = paste0(\u0026quot;RMSE = \u0026quot;, rmse_arimax)) + scale_color_identity() grid.arrange(tslm_plot, arimax_plot, nrow = 2)  \nI\u0026rsquo;m a bit surprised that the fitted TSLM model performed better than ARIMAX. I think this might vary a lot depending on the data you end up with. Both models do a decent job on the training data. But they both do a poor job at forecasting the sharp drop in movement that occurs at the end of the time series. I should point out that the ARIMAX model is forecasting the AR(1)-process recursively here, which means that there is a mean reversion where it looses its effect over time.\nSince we\u0026rsquo;re interested in the 1-step-ahead forecast, I wrote the loop below to capture what that looks like on the validation data. I hope and think that\u0026rsquo;s what I did at least! The point here is to capture the direct 1-step-ahead forecast instead of a recursive forecast.\ntsibble_se_loop \u0026lt;- data.frame() direct_pred \u0026lt;- data.frame() index \u0026lt;- 15:nrow(tsibble_se) val_weeks = 1 for(i in index) { tsibble_se_loop \u0026lt;- df_se %\u0026gt;% mutate(index = seq_along(1:nrow(.)), hits_nar_lag = lag(hits_nar), hits_beh_lag = lag(hits_beh)) %\u0026gt;% as_tsibble(., index = index) %\u0026gt;% filter(index \u0026lt; index[i]) %\u0026gt;% mutate(type = if_else(index \u0026gt; max(index) - val_weeks, \u0026quot;validation\u0026quot;, \u0026quot;training\u0026quot;)) # Save the training data tibble_train_loop \u0026lt;- tsibble_se_loop %\u0026gt;% filter(type == \u0026quot;training\u0026quot;) # Save the validation data tibble_val_loop \u0026lt;- tsibble_se_loop %\u0026gt;% filter(type == \u0026quot;validation\u0026quot;) fit_arimax \u0026lt;- tibble_train_loop %\u0026gt;% model(ARIMA(retail ~ hits_nar_lag + hits_beh_lag + pdq(1, 1, 0))) fc_arimax \u0026lt;- fit_arimax %\u0026gt;% forecast(new_data = tibble_val_loop) fc_arimax \u0026lt;- as_tibble(fc_arimax) %\u0026gt;% select(index, .mean) direct_pred \u0026lt;- rbind(direct_pred, fc_arimax) } direct_pred \u0026lt;- as_tsibble(direct_pred, index = index) arimax_direct_plot \u0026lt;- tsibble_se %\u0026gt;% mutate(color = if_else(type == \u0026quot;training\u0026quot;, \u0026quot;#7e828c\u0026quot;, \u0026quot;#7e828c\u0026quot;)) %\u0026gt;% ggplot(aes(x = index, y = retail)) + geom_line() + geom_point(aes(x = index, y = .mean), color = \u0026quot;#aa332c\u0026quot;, fill = \u0026quot;#aa332c\u0026quot;, size = 2, alpha = .8, data = direct_pred) + autolayer(direct_pred, alpha = 0.8, color = \u0026quot;#aa332c\u0026quot;) + labs( title = \u0026quot;ARIMAX model\u0026quot;, subtitle = \u0026quot;with 1-step-ahead direct forecast\u0026quot;) + scale_color_identity() + theme( axis.title.x = element_blank() ) grid.arrange(tslm_plot, arimax_plot, arimax_direct_plot, nrow = 3)  \nIt seems like the direct 1-step-ahead ARIMAX forecast does a little better than both the TSLM and the ARIMAX recursive forecast at self-correcting for the last drop off in movement. But still, it essentially fails to predict that last drop 1 week ahead. Leaving that aside, I\u0026rsquo;m a bit surprised at the level of predictive power in the model considering its simplicity.\nAll in all, this was a fun experiment.\n","date":"2021-01-31T00:00:00Z","image":"https://dfornis.github.io/dfornis/p/predicting-movement-patterns-with-r/nicolas-perondi--Ho_obgLFs4-unsplash_hudba9150e16cdf60946dabab36cfea944_1220069_120x120_fill_q75_box_smart1.jpg","permalink":"https://dfornis.github.io/dfornis/p/predicting-movement-patterns-with-r/","title":"Predicting movement patterns with R"},{"content":"I\u0026rsquo;m a master student at the department of Economic History and Global Political Economy at Stockholm University. My areas of focus are economic crises, globalization and trust. I\u0026rsquo;ve benefited a lot from others sharing their ideas and work flow in R and quantitative methods. So I thought I might share my experiences as well.\n","date":"0001-01-01T00:00:00Z","permalink":"https://dfornis.github.io/dfornis/p/about-me/","title":"About me"}]