[{"content":" Macroeconomic indicators are traditionally collected with a large lag. This limits their utility in times of high uncertainty. In a pandemic for example, infection rates may accelerate non-linearly (as seen in SIR-models) and are likely to react to variables such as temperature and degree of immunity, many of which remain unknown at the beginning of a pandemic. Nowcasting has been suggested by the Swedish Riksbank (Andersson, Reijer 2015) among others as a tool to make predictions with higher frequency data. Andersson and Reijer use monthly data from surveys, financial markets and similar sources in their nowcasting models to predict Swedish GDP. In this experiment I turned to Google Trends search data to predict movement patterns 1-week-ahead. This time horizon has proven to be relevant in the fast changing landscape of a global pandemic.\nIn practical terms, what would the utility be of predicting changes in movement patterns? A sharp drop in movement in public spaces can be categorized as a black swan event for affected parties, whether they are retail stores, public transport companies or government agencies. Predicting a sudden change in movement patterns might provide these parties with an opportunity to prepare for this change. Predicted movement patterns can also be used as a leading economic indicator.\nTheory The underlying theory has been advocated for as Narrative Economics by Robert Shiller (2017). But it has featured as a theme in many works without being mentioned by name. Its origins can be traced to the 1930\u0026rsquo;s when Keynes coined the phrase animal spirits to capture a characteristic of human behavior beyond what was imagined in the classical models of economics.\nThe assumption is that economic outcomes, to some extent, are a function of the stories and ideas people tell themselves and others. When these stories reach a wide and receptive audience they turn economic behavior into heard behavior. If the narratives that occupy conversations and the minds of people can be measured, they can theoretically be used to predict behavior.\nData To capture the narrative component I used an R library called gtrendsR which runs the API call to Google Trends so you don\u0026rsquo;t have to cURL it manually. The data is slightly unreliablein the sense that Google provides the amount of hits as an index which is calculated in an unknown black box of magic. The index does however seem to measure change in search trends over a shorter time horizon reasonably well.\nThe query used in Google Trends was covid and virus. As the interest in Covid-19 rises, people are expected to go online to search for information about the virus or the situation which registers in the index. This variable is countercyclical and should be negatively correlated with movement patterns.\nA different and procyclical variable was used as well. For this variable my query was snälltåget and sj which are the main operators of long distance trains in Sweden. The assumption here is that people on average go online and search for train tickets 1 week ahead of departure. This variable should be positively correlated with movement patterns.\nFor data on movement patterns I used Google Mobility Report. It was launched in the infancy of the Covid-19 pandemic to track changes in movement patterns. It calculates changes from the same days baseline categorized by country, sub region and type of location/activity (retail and recreation, parks, homes etc).\nModel One of the problems with applying a simple linear regression model is that time series data are likely to be autocorrelated which will express itself as covariance between residuals. One way of resolving this is to model the residuals as an ARIMA-process. I ended up with a (1,1,0) process here. So the model used will be a linear regression with ARIMA errors. Sometimes referred to as ARIMAX, where the X denotes an external regressor.\nThe equation:\n$Y_t = B_1X_{1t} + B_2X_{2t} + n_t$ where $n_t = \\phi n_{t-1} + \\epsilon _t$ is the ARIMA error term. Notations for differencing are missing.\nIn the model, narratives are spread at time t and have an effect on economic behavior at time t+1. Having input variables lagged at t+1 allows the use of an external regressor as fresh input for 1-step-ahead predictions in an ARIMAX model.\nR code Libraries:\nlibrary(tidyverse) library(gtrendsR) # for Google Trends API calls. library(fable) # tidyverse compatible replacement of the forecast package. library(feasts) library(tsibble) library(lubridate) # to help with some weekly time series strangeness. These were the inputs in the Google Trends API call.\nquery \u0026lt;- c(\u0026quot;covid\u0026quot;, \u0026quot;virus\u0026quot;) query2 \u0026lt;- c(\u0026quot;snälltåget\u0026quot;, \u0026quot;sj\u0026quot;) date \u0026lt;- c(\u0026quot;2020-03-01 2021-01-15\u0026quot;) I wrote a function which loops along the query vectors and outputs the mean of hits (index of times searched) in a data frame. This allows for easy experimentation with different search queries and explorative data analysis.\nsearch_se \u0026lt;- data.frame() search_se_mean \u0026lt;- function(query) { for(i in seq_along(search)) { print(search[i]) search_se \u0026lt;- rbind(covid_se, gtrends(keyword = search[i], geo = \u0026quot;SE\u0026quot;, time = date)[[1]]) } search_se_mean \u0026lt;- search_se %\u0026gt;% mutate(week = yearweek(date, week_start = 1)) %\u0026gt;% group_by(week) %\u0026gt;% summarise(hits = mean(hits)) } gtrends_nar \u0026lt;- search_se_mean(query) gtrends_beh \u0026lt;- search_se_mean(query2) gtrends_se \u0026lt;- gtrends_nar %\u0026gt;% full_join(gtrends_beh, by = \u0026quot;week\u0026quot;, suffix = c(\u0026quot;_nar\u0026quot;, \u0026quot;_beh\u0026quot;)) Plotted together:\n \nHits for train travel drops sharply, as one would expect, and then rebounds over the summer. Hits for the virus jumps up but starts dropping surprisingly fast. Lower levels over the summer is in line with lower spread. The index jumps up again in the autumn of 2020. This inverse relationship between narrative and behavioral predictors made intuitive sense and looked promising.\nThe next step was to find out if the predictors had any explanatory power on the leading indicator: movement patterns.\nThe movement data is available at https://www.google.com/covid19/mobility/ I used the .csv-file for global data and did the filtering for my region of interest in R. Once it\u0026rsquo;s loaded, the following code will filter for the target country with country_region_code. I filtered for sub_region_1 = \u0026quot;\u0026quot;  in order to capture data for all of Sweden. The data is then transformed into weeks. Note that data for retail_and_recreation_percent_change_from_baseline was used. The movement patterns being predicted are in retail and recreation areas.\ngmr_se \u0026lt;- gmr %\u0026gt;% mutate(date = as_date(date)) %\u0026gt;% filter(country_region_code == \u0026quot;SE\u0026quot;, sub_region_1 == \u0026quot;\u0026quot;, date \u0026gt;= \u0026quot;2020-03-01\u0026quot; \u0026amp; date \u0026lt;= \u0026quot;2021-01-17\u0026quot;) %\u0026gt;% select(date, retail_and_recreation_percent_change_from_baseline) %\u0026gt;% mutate(week = yearweek(date, week_start = 1)) %\u0026gt;% group_by(week) %\u0026gt;% summarise(across(everything(), list(mean))) %\u0026gt;% rename( \u0026quot;retail\u0026quot; = retail_and_recreation_percent_change_from_baseline_1) %\u0026gt;% select(week, retail) df_se \u0026lt;- gtrends_se %\u0026gt;% # join data sets together left_join(gmr_se) df_se$retail \u0026lt;- df_se$retail + 100 # for potential differencing and log transformations Regressions and scatter plots:  \nInspecting the plots reveals that there is a linear relationship between the variables. The virus search variable does well with 1 lag, while the train ticket search does better without a lag. This will likely depend a lot on the search queries used and on the reliability of Google\u0026rsquo;s black box data. For the purpose of this experiment I stuck with the theory in order to be able to predict 1-step-ahead and continued with lagged variables.\nNext, preparing the data for model fitting.\nval_weeks \u0026lt;- 15 #15 weeks for validating the models df_se_index \u0026lt;- df_se %\u0026gt;% mutate(index = seq_along(1:nrow(.)), type = if_else(index \u0026gt; max(index) - val_weeks, \u0026quot;validation\u0026quot;, \u0026quot;training\u0026quot;), hits_nar_lag = lag(hits_nar), hits_beh_lag = lag(hits_beh)) tsibble_se \u0026lt;- as_tsibble(df_se_index, index = index) # as time series tibble tsibble_se_train \u0026lt;- tsibble_se %\u0026gt;% filter(type == \u0026quot;training\u0026quot;) tsibble_se_val \u0026lt;- tsibble_se %\u0026gt;% filter(type == \u0026quot;validation\u0026quot;) Computing both the TSLM and the ARIMAX (1,1,0) model to see if it makes sense to model the residuals as an ARIMA-process.\nfit_tslm \u0026lt;- tsibble_se_train %\u0026gt;% # fit the model on the training data model(TSLM(retail ~ hits_nar_lag + hits_beh_lag)) fc_tslm \u0026lt;- fit_tslm %\u0026gt;% # forecast with the validation data forecast(new_data = tsibble_se_val) fit_arimax \u0026lt;- tsibble_se_train %\u0026gt;% model(ARIMA(retail ~ hits_nar_lag + hits_beh_lag + pdq(1, 1, 0))) fc_arimax \u0026lt;- fit_arimax %\u0026gt;% forecast(new_data = tsibble_se_val) rmse_tslm \u0026lt;- round(accuracy(fit_tslm)[, 4], digits = 2) # extract RMSE rmse_arimax \u0026lt;- round(accuracy(fit_arimax)[, 4], digits = 2) # extract RMSE fit_tslm %\u0026gt;% gg_tsresiduals() The patterns at index \u0026gt; 18 or so didn\u0026rsquo;t look like a white noise-process. This shows up in the ACF plot as well, even though the spikes aren\u0026rsquo;t significant. The takeaway is that modeling the residuals as an ARIMA-process might prove useful.  TSLM residuals \nFitting the ARIMAX-model:\n ARIMAX residuals  Evaluating the ARIMAX-residuals, they looked more like a stationary white noise process. No significant spikes.\nPlotting the models and forecasts produced with Fable:\ntslm_plot \u0026lt;- tsibble_se %\u0026gt;% mutate(color = if_else(type == \u0026quot;training\u0026quot;, \u0026quot;#7e828c\u0026quot;, \u0026quot;#7e828c\u0026quot;)) %\u0026gt;% ggplot(aes(x = index, y = retail)) + geom_line() + autolayer(fc_tslm, alpha = 0.5, color = \u0026quot;#aa332c\u0026quot;) + geom_line(aes(color = color), alpha = 0.8) + geom_line(aes(y = .fitted, color = \u0026quot;#aa332c\u0026quot;), data = augment(fit_tslm)) + labs( title = \u0026quot;TSLM model\u0026quot;, subtitle = paste0(\u0026quot;RMSE = \u0026quot;, rmse_tslm)) + scale_color_identity() arimax_plot \u0026lt;- tsibble_se %\u0026gt;% mutate(color = if_else(type == \u0026quot;training\u0026quot;, \u0026quot;#7e828c\u0026quot;, \u0026quot;#7e828c\u0026quot;)) %\u0026gt;% ggplot(aes(x = index, y = retail)) + geom_line() + autolayer(fc_arimax, alpha = 0.5, color = \u0026quot;#aa332c\u0026quot;) + geom_line(aes(color = color), alpha = 0.8) + geom_line(aes(y = .fitted, color = \u0026quot;#aa332c\u0026quot;), data = augment(fit_arimax)) + labs( title = \u0026quot;ARIMAX model\u0026quot;, subtitle = paste0(\u0026quot;RMSE = \u0026quot;, rmse_arimax)) + scale_color_identity() grid.arrange(tslm_plot, arimax_plot, nrow = 2)  \nI\u0026rsquo;m a bit surprised that the fitted TSLM model performed in parity with the ARIMAX model. I think this might vary a lot depending on the data you end up with. Both models do a decent job on the training data. But they both do a poor job at forecasting the sharp drop in movement that occurs at the end of the time series. I should point out that the ARIMAX model is forecasting the AR(1)-process recursively here. Consequently, there is a mean reversion and it looses its effect over time.\nSince I\u0026rsquo;m interested in the 1-step-ahead forecast, I wrote the loop below to capture what that looks like on the validation data. The point here is to capture the direct 1-step-ahead forecast instead of a recursive forecast, and thereby utilize the full potential of the ARIMAX-model.\ntsibble_se_loop \u0026lt;- data.frame() direct_pred \u0026lt;- data.frame() index \u0026lt;- 15:nrow(tsibble_se) val_weeks = 1 for(i in index) { tsibble_se_loop \u0026lt;- df_se %\u0026gt;% mutate(index = seq_along(1:nrow(.)), hits_nar_lag = lag(hits_nar), hits_beh_lag = lag(hits_beh)) %\u0026gt;% as_tsibble(., index = index) %\u0026gt;% filter(index \u0026lt; index[i]) %\u0026gt;% mutate(type = if_else(index \u0026gt; max(index) - val_weeks, \u0026quot;validation\u0026quot;, \u0026quot;training\u0026quot;)) # Save the training data tibble_train_loop \u0026lt;- tsibble_se_loop %\u0026gt;% filter(type == \u0026quot;training\u0026quot;) # Save the validation data tibble_val_loop \u0026lt;- tsibble_se_loop %\u0026gt;% filter(type == \u0026quot;validation\u0026quot;) fit_arimax \u0026lt;- tibble_train_loop %\u0026gt;% model(ARIMA(retail ~ hits_nar_lag + hits_beh_lag + pdq(1, 1, 0))) fc_arimax \u0026lt;- fit_arimax %\u0026gt;% forecast(new_data = tibble_val_loop) fc_arimax \u0026lt;- as_tibble(fc_arimax) %\u0026gt;% select(index, .mean) direct_pred \u0026lt;- rbind(direct_pred, fc_arimax) } direct_pred \u0026lt;- as_tsibble(direct_pred, index = index) arimax_direct_plot \u0026lt;- tsibble_se %\u0026gt;% mutate(color = if_else(type == \u0026quot;training\u0026quot;, \u0026quot;#7e828c\u0026quot;, \u0026quot;#7e828c\u0026quot;)) %\u0026gt;% ggplot(aes(x = index, y = retail)) + geom_line() + geom_point(aes(x = index, y = .mean), color = \u0026quot;#aa332c\u0026quot;, fill = \u0026quot;#aa332c\u0026quot;, size = 2, alpha = .8, data = direct_pred) + autolayer(direct_pred, alpha = 0.8, color = \u0026quot;#aa332c\u0026quot;) + labs( title = \u0026quot;ARIMAX model\u0026quot;, subtitle = \u0026quot;with 1-step-ahead direct forecast\u0026quot;) + scale_color_identity() + theme( axis.title.x = element_blank() ) grid.arrange(tslm_plot, arimax_plot, arimax_direct_plot, nrow = 3)  \nThe direct 1-step-ahead ARIMAX forecast does a little better than both the TSLM and the ARIMAX recursive forecast at self-correcting for the last drop off in movement. But still, it essentially fails to predict that last drop 1 week ahead. Leaving that aside, the level of predictive power in the model is surprising considering its simplicity. What I take away from this experiment is that with a bit of fine-tuning and perhaps added complexity it is possible to predict a leading economic indicator with high-frequency narrative data gathered Google Trends.\nThe highest requirements for statistical rigor does not necessarily apply in a forecasting model, in the same way it does when exploring a casual relationship. Given that, it would be interesting to apply a machine-learning technique for nowcasting with a larger and more varied narrative data set.\nReferences Andersson, Michael K. and Reijer, Ard H.J. (2015), “Nowcasting”, Sveriges Riksbank Economic Review, 2015:1, Sveriges Riksbank, pp. 75-89.\nShiller, J.R. (2017) Narrative Economics. NBER Working Paper, No. 23075.\n","date":"2021-01-31T00:00:00Z","image":"https://dfornis.github.io/dfornis/p/nowcasting-a-leading-economic-indicator-in-times-of-uncertainty-with-r/nicolas-perondi--Ho_obgLFs4-unsplash_hudba9150e16cdf60946dabab36cfea944_1220069_120x120_fill_q75_box_smart1.jpg","permalink":"https://dfornis.github.io/dfornis/p/nowcasting-a-leading-economic-indicator-in-times-of-uncertainty-with-r/","title":"Nowcasting a leading economic indicator in times of uncertainty - with R"}]