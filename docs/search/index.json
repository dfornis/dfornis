[{"content":"s\nIntroduction Randomly assigned controlled experiments are, naturally, a favored tool by scientists who want to make claims about causality in research. They are however expensive, impractical and often ethically dubious to run in a social science or policy context. Consider the country that randomly assigns its regions into a treatment and control group when implementing criminal justice reform, in order to find out if it has the intended effect, all else equal. Such an exercise would result in a few excited scientists and a lot of angry citizens. Until recently, quantitative social scientists made do with running simple OLS regressions on observational data - reporting correlations as the primary result. This has changed for with the introduction of a set of methods that help researchers identify random assignment of treatment in existing data that, with a bit of creativity, makes it possible to construct artificial treatment and control groups without running an actual experiment.\nCollectively, the introduction of these models has resulted in what has been dubbed a \u0026lsquo;credibility revolution\u0026rsquo; in economics, and a shared Nobel prize. Guido Imbens, one of the main forces behind these methods, has described a late comer to the group - the synthethic control method - as “arguably the most important innovation in the policy evaluation literature in the last 15 years” (Athey \u0026amp; Imbens, 2017). In this text, I will provide a practical example of how this method can be applied on a real and current policy case from the energy policy sector. The text is concise write up of the evaluation plan I helped develop for Sweden\u0026rsquo;s evaluation of its excise tax exemption for high-blended biofuels, as requested by the European Commission. It also provides an implementation on actual data with preliminary results.\nThe synthethic control method The method was developed in a series of papers (Abadie and Gardeazabal 2003, and Abadie, Diamond and Hainmueller 2010) and has become an established method for policy evaluations in case studies with one treated unit. Its first policy application was on proposition 99 - an increase of the excise tax on tobacco products in California - by testing its impact on smoking. Politics are not designed around the conditions of randomly controlled trials, meaning that there were no \u0026ldquo;control California\u0026rdquo; available that didn\u0026rsquo;t get the treatment while also sharing California\u0026rsquo;s other characteristics. As such, there was no obvious way to estimate the effect of the legislation, all else equal. The synthetic control method solves this rather elegantly. It offers an algorithmic approach to constructing control groups. For the unit that receives treatment, the control is a weighted average of the untreated units that best predicts the outcome variable in the pre-treatment period for the treated unit. In the post-treatment period, it provides a synthetic counterfactual for the treated unit. In the case of tobacco legislation, the synthetic control weighs the predictors of smoking (drinking, demographic variables, etc.) from a panel dataset of states, and creates a synthetic control from donor states that best match the rate of smoking of California, for the years that precede the excise tax change.\n \nAfter the policy intervention (post-treatment), the rate of smoking is allowed to deviate, given that there is an effect. The difference between the rate of smoking in actual California and the synthetic California, in the post-treatment period, is the estimated effect of an excise tax increase on smoking.\nThe case of Sweden\u0026rsquo;s excise tax exemption Sweden introduced an excise tax exemption for high-blended and pure biofuels in two steps, first by removing the energy tax in 2003 and then the carbon tax in 2008. High-blended biofuels include bioethanol based fuels such as E85 and bio/synthethic diesels such as FAME and HVO. The price at pump is often higher for these fuels than their fossil equivelants, even without excise taxes. This limits the impact on fuels used in the passenger fleet, but is enough to incentivize transport and public transit companies into switching by providing them with the value of environmental signaling and ability to meet carbon emission targets. Ultimately, the goal of incentivizing biofuel use is to reduce carbon emissions from transport.\nPreparations So how should we go about constructing the synthetic control? First, we need to know what countries, in the EU, that are untreated, i.e. had no excise tax exemption in place, during the studied period.\n \nI did a binary coding of EU country\u0026rsquo;s excise tax policies instructed by state aid case documents in the European Commission\u0026rsquo;s state aid database. This provided us with a set of untreated countries, that can be used as donors for the synthetic control. Now, the model needs data on predictors for, in this case, carbon emissions from transport per capita as well as the actual outcome variable in order to estimate its weights. We were instructed in choosing predictor variables by Andersson (2019) who uses a synthetic control model to estimate the effect of the original introduction of a carbon tax on transport fuels in 1990. The actual data on predictors was obtained from The World Bank database, and for the outcome variable, we obtained panel data for a large selection of countries for 1990 through 2019 from the Climate Watch carbon emissions database. First, let\u0026rsquo;s consider what would happen if we used the average of carbon emissions from transport from relevant countries as a control group, without weights.\n \nIt\u0026rsquo;s clear that other, but not all, countries in this group experienced falling carbon emissions from transport in the late 2000\u0026rsquo;s and on. A simple explanation is that all countries follow the EU Renewable Energy Directive and similar directives that preceded it with biofuel targets for the transport sectors, pursuing different policy tools for this purpose. Some applied obligatory blending quotas for biofuels in gasoline and diesel. The discrepancy is quiet large, both in absolute terms and in rate of change. It seems like there are confounding factors that muddle our ability to single out the effect of a single policy intervention, such as an excise tax change. Further, underlying factors such as road networks, GDP, fuel taxes and more are making it hard to find a suitable control country, similar to Sweden but without the excise tax change, and with all else equal. Using the average co2 emissions from transport of all countries, as a control, wouldn\u0026rsquo;t help control for confounders.\n \nBut what if, instead of an average, we could estimate weights for these countries, as determined by outcome and predictor variables, in order to create a synthetic control with a better fit. Having the lines fit, in the pre-treatment period, would indicate that both observed and unobserved confounding variables are controlled for.\nEstimating the weights The synthetic control is constructed by optimizing two set of weights. It\u0026rsquo;s an optimization problem that seeks to minimize the sum of the absolute differences between the outcome variable for the treated unit (Sweden) and the donor countries.\nThe first component is a vector of the average of the predictor variables for the treated unit over the pre-intervention period given by: $(X1 = (1/T0) * Σ_{t=1}^{T0} x1(t))$\nThe second component is a matrix of the average of the predictor variables for each potential control unit over the pre-intervention period given by: $(X1 = (1/T0) * Σ_{t=1}^{T0} x1(t))$\nAnd the optimization problem can be defined as: $(\\min_W \\quad || X_1 - X_0 W || \\text{subject to } W \\geq 0, \\quad \\sum_{j=1}^{J} W_j = 1)$\nHere, W is a vector of weights for the control units that minimizes the difference between the treated unit and the synthetic control unit in the pre-intervention period. Each weight in W corresponds to one control unit (one country in this case). The constraints that W ≥ 0 and Σ W = 1 ensure that the weights are non-negative and sum to 1.\nThe predictor variable weights are implicitly and recursively determined in this process, so as to minimize the difference between the outcome variable for Sweden and the synthetic control.\nHere are the predictor values that I obtained:\n \nWe can see that it optimized for a close match between Sweden (Treated) and the synthetic control in terms of diesel price, passenger cars per 1000 inhabitants and previous carbon emissions from transport, indicative of that these were strong predictors of the outcome variable.\nIn terms of country weights, I obtained the following:\n \nThe algorithm determined that France, Finland and Norway were the most suitable donor countries.\nResults After the synthetic control is constructed, we compare the outcome variable for the treated unit and the synthetic control during the post-intervention period. Any difference between these two could be attributed to the treatment. Granted that the synthetic control has a good fit with the actual outcome for Sweden, chances are that it\u0026rsquo;s a good counterfactual.\nIn terms of fit, we went from the average of all countries:   to the weighted synthetic control:  \nWe can see that the pre-treatment period provides a decent but not perfect fit, at least controlling for parts of the variation that stems from covariates such as GDP, diesel price, and passenger car density, as well as unobserved covariates. In the pst-treatment period, we observe a gradual divergence between the actual carbon emissions for Sweden and the synthetic control which provides us with a counterfactual scenario where Sweden did not implement an excise tax policy change.\n","date":"2023-06-14T00:00:00Z","image":"https://dfornis.github.io/dfornis/p/ex-post-policy-evaluation-with-the-synthethic-control-method/background_hu52029d49fd37a8ac8a9facd10a587a5d_1092055_120x120_fill_box_smart1_2.png","permalink":"https://dfornis.github.io/dfornis/p/ex-post-policy-evaluation-with-the-synthethic-control-method/","title":"Ex-post policy evaluation with the synthethic control method"},{"content":"Background How long is the wait for a rental apartment in Stockholm? It\u0026rsquo;s an engaging question in the spotlight of both politics and daily life in the Swedish capital. In light of a recent surge in news articles about how young swedes struggle to enter the housing market, I took a look at data provided by Stockholms Bostadskö - Stockholm\u0026rsquo;s sole distributor of rental apartments. It tells a story about a deteriorating rental market with a rapidly growing wait time for apartments (Swedes queue for apartments: Soviet style). I recently found the time to send them an email and request data going back 10 years.\nThe plot  \nVisualizing the data, I was struck by how the distribution of the number of years people wait for their apartments has shifted to the right. In another thread on this forum, a contributor had suggested that anecdotal experiences of longer wait times are imagined. \u0026ldquo;It has always been like this and every generation rediscovers the dysfunctional nature of the rental market,\u0026rdquo; his or her sentiment echoed. Looking at the data, I knew that this wasn\u0026rsquo;t true: there had been a dramatic shift for the worse in less than ten years. I use a bar plot to capture this change, colorizing the years of 2010 and 2019 to emphasize the shift. I also want to provide viewers with data for the intermittent years. Shown in a gray scale, the intermittent bars don\u0026rsquo;t obfuscate the two colorized distributions and provide more information to curious viewers.\nDelivery As is often the case in a different subreddit, r/dataisbeautiful, a visualization can garner an audience by itself if it\u0026rsquo;s a map or if it\u0026rsquo;s an animated plot. My experience however is that a more technical plot ideally should be accompanied by a story or a striking fact. To this effect, the title of my post contains information that is deducible from the plot but not immediately available from looking at it. The title of my post states that, \u0026ldquo;In 2010 65% of applicants attained their rental apartment within 6 years of waiting. In 2019, this number had fallen to 11.7%.\u0026rdquo; This is a striking way to describe the cumulative distributional shift with data that is available in the visualization. It\u0026rsquo;s releatable, considering that Swedes think of six years as a reasonable time to wait for an apartment, and it instructs the viewer of what kind of information one can obtain from looking at the plot.\n20,000 might not make the bar for going viral on Tik Tok. But these are 20,000 politically interested Swedish members of a subreddit. Why did this plot strike a chord with its users? A pattern emerges from surveying the topics that tend to garner upvotes and comments from visitors to this subreddit. Inflation, electricity prices, the housing market, corruption, petty crime such as theft, and growing inequality are topics that garner the most attention. They are political and topical while simultaneously impacting the quality of life of its users. Dysfunctional housing and rental markets are issues that many westerners find themselves navigating as young adults. It affects their quality of life in a tangible and often negative way. Data, effectively communicated, that affirm the personal experience of not being able to attain a rental apartment, is engaging. A plot that describes this reality is not only saying: \u0026ldquo;please worry about this\u0026rdquo;, but it also affirms the lived experience of the viewer.\nSummary To summarize, these are ingredients that I believe helps a more technical plot garner a large audience:\n Data that connects with the viewer on a personal level Data that confirms (or contradicts (both are probably engaging)) the lived experience Data that signals that someone\u0026rsquo;s status should decrease (politicians in this case) or increase. This is a Tyler Cowen-esque speculation Effectively communicates a shift in time or a discrepancy between groups Delivered with a story or a striking fact that is deducible from the visualized data  ","date":"2022-09-12T00:00:00Z","image":"https://dfornis.github.io/dfornis/p/what-makes-a-plot-go-semi-viral/bostadsmarknad_en_bw_hu31aecfb434fea470d7b7e4694eb4f9ac_103821_120x120_fill_box_smart1_2.png","permalink":"https://dfornis.github.io/dfornis/p/what-makes-a-plot-go-semi-viral/","title":"What makes a plot go (semi)-viral?"},{"content":"While initially hesitant, owing to a sense that I would rather be involved in actual research, visualizing data turned out to be a creative outlet in the midst of more heavy data wrangling and theoretical work.\nAll visualizations were made in R with ggplot2.\nClimate adaption The first set of visualizations were made from a large panel dataset on the climate adaption activities of international organizations, developed for a PHD thesis by Ece Kural. A lesson I drew from this was to include less text in the subtitle.\n \nThis simple line graph of aggregated values by region was complemented by a facet of all organizations, maintaining some overview and comparability while displaying a lot of data in one visualization.\n \nFinally I tapped into the animated graph craze with a gif. I didn\u0026rsquo;t manage to iron out all the kinks before time was better invested in other projects.\n \nLegitimacy gap I was asked to coauthor a background paper with Assoc. Prof. Lisa Dellmuth on the gap in legitimacy beliefs (trust) between citizens and elites towards international organizations. The main research was already done but in order to tailor the paper for the UN Stockholm50+ conference, I got to work with the underlying data in order to analyze it along cross sections that were more relevant to this context.\nAlong the way I made a few simple visualizations thematically close to those commonly found in The Economist. By this point I pivoted towards the style and formatting used in business news outlets for increased clarity and readability.\n \nOther It\u0026rsquo;s challenging to communicate findings for multiple variables from panel data along a uniform y or x-scale. Here I used a dot plot and averaged the values with the year 2000 as a divider. This was for book project by Lisa Dellmuth on inequalities within EU regions.\n \n","date":"2022-04-05T00:00:00Z","image":"https://dfornis.github.io/dfornis/p/data-visualizations-of-2021/CCA_line_header_hu15de3ee573abedaca1c2792f2cd1cbb8_206024_120x120_fill_box_smart1_2.png","permalink":"https://dfornis.github.io/dfornis/p/data-visualizations-of-2021/","title":"Data visualizations of 2021"},{"content":" Macroeconomic indicators are traditionally collected with a large lag. This limits their utility in times of high uncertainty. In a pandemic for example, infection rates may accelerate non-linearly (as seen in SIR-models) and are likely to react to variables such as temperature and degree of immunity, many of which remain unknown at the beginning of a pandemic. Nowcasting has been suggested by the Swedish Riksbank (Andersson, Reijer 2015) among others as a tool to make predictions with higher frequency data. Andersson and Reijer use monthly data from surveys, financial markets and similar sources in their nowcasting models to predict Swedish GDP. In this experiment I turned to Google Trends search data to predict movement patterns 1-week-ahead. This time horizon has proven to be relevant in the fast changing landscape of a global pandemic.\nIn practical terms, what would the utility be of predicting changes in movement patterns? A sharp drop in movement in public spaces can be categorized as a black swan event for affected parties, whether they are retail stores, public transport companies or government agencies. Predicting a sudden change in movement patterns might provide these parties with an opportunity to prepare for this change. Predicted movement patterns can also be used as a leading economic indicator.\nTheory The underlying theory has been advocated for as Narrative Economics by Robert Shiller (2017). But it has featured as a theme in many works without being mentioned by name. Its origins can be traced to the 1930\u0026rsquo;s when Keynes coined the phrase animal spirits to capture a characteristic of human behavior beyond what was imagined in the classical models of economics.\nThe assumption is that economic outcomes, to some extent, are a function of the stories and ideas people tell themselves and others. When these stories reach a wide and receptive audience they turn economic behavior into heard behavior. If the narratives that occupy conversations and the minds of people can be measured, they can theoretically be used to predict behavior.\nData To capture the narrative component I used an R library called gtrendsR which runs the API call to Google Trends so that there\u0026rsquo;s no need to cURL it manually. The data is slightly unreliable in the sense that Google provides the amount of hits as an index which is calculated in an unknown black box of magic. The index does however seem to measure changes in search trends over a shorter time horizon reasonably well.\nThe query used in Google Trends was covid and virus. As the interest in Covid-19 rises, people are expected to go online to search for information about the virus or the situation as it unfolds which registers in the index. This variable is countercyclical and should be negatively correlated with movement patterns.\nA procyclical variable was used as well. For this variable my query was snälltåget and sj which are the main operators of long distance trains in Sweden. The assumption here is that people on average go online and search for train tickets 1 week ahead of departure. This variable should be positively correlated with movement patterns.\nFor data on movement patterns I used Google Mobility Report. It was launched in the infancy of the Covid-19 pandemic to track changes in movement patterns. It calculates changes from the same days baseline categorized by country, sub region and type of location/activity (retail and recreation, parks, homes etc).\nModel One of the problems with applying a simple linear regression model is that time series data are likely to be autocorrelated which will express itself as covariance between residuals. One way of resolving this is to model the residuals as an ARIMA-process. I ended up with a (1,1,0) process here. So the model used will be a linear regression with ARIMA errors. Sometimes referred to as ARIMAX, where the X denotes an external regressor.\nThe equation:\n$Y_t = B_1X_{1t} + B_2X_{2t} + n_t$ where $n_t = \\phi n_{t-1} + \\epsilon _t$ is the ARIMA error term. Notations for differencing are missing.\nIn the model, narratives are spread at time t and have an effect on economic behavior at time t+1. Having input variables lagged at t+1 allows the use of an external regressor as fresh input for 1-step-ahead predictions in an ARIMAX model.\nR code Libraries:\nlibrary(tidyverse) library(gtrendsR) # for Google Trends API calls. library(fable) # tidyverse compatible replacement of the forecast package. library(feasts) library(tsibble) library(lubridate) # to help with some weekly time series strangeness. These were the inputs in the Google Trends API call.\nquery \u0026lt;- c(\u0026quot;covid\u0026quot;, \u0026quot;virus\u0026quot;) query2 \u0026lt;- c(\u0026quot;snälltåget\u0026quot;, \u0026quot;sj\u0026quot;) date \u0026lt;- c(\u0026quot;2020-03-01 2021-01-15\u0026quot;) I wrote a function which loops along the query vectors and outputs the mean of hits (index of times searched) in a data frame. This allows for easy experimentation with different search queries and explorative data analysis.\nsearch_se \u0026lt;- data.frame() search_se_mean \u0026lt;- function(query) { for(i in seq_along(search)) { print(search[i]) search_se \u0026lt;- rbind(covid_se, gtrends(keyword = search[i], geo = \u0026quot;SE\u0026quot;, time = date)[[1]]) } search_se_mean \u0026lt;- search_se %\u0026gt;% mutate(week = yearweek(date, week_start = 1)) %\u0026gt;% group_by(week) %\u0026gt;% summarise(hits = mean(hits)) } gtrends_nar \u0026lt;- search_se_mean(query) gtrends_beh \u0026lt;- search_se_mean(query2) gtrends_se \u0026lt;- gtrends_nar %\u0026gt;% full_join(gtrends_beh, by = \u0026quot;week\u0026quot;, suffix = c(\u0026quot;_nar\u0026quot;, \u0026quot;_beh\u0026quot;)) Plotted together:\n \nHits for train travel dropped sharply, as one would expect, and then rebounded over the summer. Hits for the virus jumped up initially but dropped surprisingly fast. Lower levels over the summer was in line with a lower spread of the virus. The index jumped up again in the autumn of 2020, in line with a rising spread of the virus. This inverse relationship between the train travel and virus narrative predictors made intuitive sense and looked promising.\nThe next step was to find out if the predictors had any explanatory power on the leading indicator: movement patterns.\nThe movement data is available at https://www.google.com/covid19/mobility/ I used the .csv-file for global data and did the filtering for my region of interest in R. Once it\u0026rsquo;s loaded, the following code will filter for the target country with country_region_code. I filtered for sub_region_1 = \u0026quot;\u0026quot;  in order to capture data for all of Sweden. The data is then transformed into weeks. Note that data for retail_and_recreation_percent_change_from_baseline was used. The movement patterns being predicted are in retail and recreation areas.\ngmr_se \u0026lt;- gmr %\u0026gt;% mutate(date = as_date(date)) %\u0026gt;% filter(country_region_code == \u0026quot;SE\u0026quot;, sub_region_1 == \u0026quot;\u0026quot;, date \u0026gt;= \u0026quot;2020-03-01\u0026quot; \u0026amp; date \u0026lt;= \u0026quot;2021-01-17\u0026quot;) %\u0026gt;% select(date, retail_and_recreation_percent_change_from_baseline) %\u0026gt;% mutate(week = yearweek(date, week_start = 1)) %\u0026gt;% group_by(week) %\u0026gt;% summarise(across(everything(), list(mean))) %\u0026gt;% rename( \u0026quot;retail\u0026quot; = retail_and_recreation_percent_change_from_baseline_1) %\u0026gt;% select(week, retail) df_se \u0026lt;- gtrends_se %\u0026gt;% # join data sets together left_join(gmr_se) df_se$retail \u0026lt;- df_se$retail + 100 # for potential differencing and log transformations Regressions and scatter plots:  \nInspecting the plots revealed that there was a linear relationship between the variables. The virus search variable did well with 1 lag, while the train ticket search did better without a lag. This will likely depend a lot on the search queries used and on the reliability of Google\u0026rsquo;s black box data. For the purpose of this experiment I stuck with the theory in order to be able to predict 1-step-ahead and continued with lagged variables.\nNext, preparing the data for model fitting.\nval_weeks \u0026lt;- 15 #15 weeks for validating the models df_se_index \u0026lt;- df_se %\u0026gt;% mutate(index = seq_along(1:nrow(.)), type = if_else(index \u0026gt; max(index) - val_weeks, \u0026quot;validation\u0026quot;, \u0026quot;training\u0026quot;), hits_nar_lag = lag(hits_nar), hits_beh_lag = lag(hits_beh)) tsibble_se \u0026lt;- as_tsibble(df_se_index, index = index) # as time series tibble tsibble_se_train \u0026lt;- tsibble_se %\u0026gt;% filter(type == \u0026quot;training\u0026quot;) tsibble_se_val \u0026lt;- tsibble_se %\u0026gt;% filter(type == \u0026quot;validation\u0026quot;) Computing both the TSLM and the ARIMAX (1,1,0) model to see if it makes sense to model the residuals as an ARIMA-process.\nfit_tslm \u0026lt;- tsibble_se_train %\u0026gt;% # fit the model on the training data model(TSLM(retail ~ hits_nar_lag + hits_beh_lag)) fc_tslm \u0026lt;- fit_tslm %\u0026gt;% # forecast with the validation data forecast(new_data = tsibble_se_val) fit_arimax \u0026lt;- tsibble_se_train %\u0026gt;% model(ARIMA(retail ~ hits_nar_lag + hits_beh_lag + pdq(1, 1, 0))) fc_arimax \u0026lt;- fit_arimax %\u0026gt;% forecast(new_data = tsibble_se_val) rmse_tslm \u0026lt;- round(accuracy(fit_tslm)[, 4], digits = 2) # extract RMSE rmse_arimax \u0026lt;- round(accuracy(fit_arimax)[, 4], digits = 2) # extract RMSE fit_tslm %\u0026gt;% gg_tsresiduals() The patterns at index \u0026gt; 18 or so didn\u0026rsquo;t look like a white noise-process. This shows up in the ACF plot as well, even though the spikes aren\u0026rsquo;t significant. The takeaway is that modeling the residuals as an ARIMA-process might prove useful.  TSLM residuals \nFitting the ARIMAX-model below. Evaluating the residuals, they looked more like a stationary white noise process. No significant spikes.\n ARIMAX residuals \nPlotting the models and forecasts produced with Fable:\ntslm_plot \u0026lt;- tsibble_se %\u0026gt;% mutate(color = if_else(type == \u0026quot;training\u0026quot;, \u0026quot;#7e828c\u0026quot;, \u0026quot;#7e828c\u0026quot;)) %\u0026gt;% ggplot(aes(x = index, y = retail)) + geom_line() + autolayer(fc_tslm, alpha = 0.5, color = \u0026quot;#aa332c\u0026quot;) + geom_line(aes(color = color), alpha = 0.8) + geom_line(aes(y = .fitted, color = \u0026quot;#aa332c\u0026quot;), data = augment(fit_tslm)) + labs( title = \u0026quot;TSLM model\u0026quot;, subtitle = paste0(\u0026quot;RMSE = \u0026quot;, rmse_tslm)) + scale_color_identity() arimax_plot \u0026lt;- tsibble_se %\u0026gt;% mutate(color = if_else(type == \u0026quot;training\u0026quot;, \u0026quot;#7e828c\u0026quot;, \u0026quot;#7e828c\u0026quot;)) %\u0026gt;% ggplot(aes(x = index, y = retail)) + geom_line() + autolayer(fc_arimax, alpha = 0.5, color = \u0026quot;#aa332c\u0026quot;) + geom_line(aes(color = color), alpha = 0.8) + geom_line(aes(y = .fitted, color = \u0026quot;#aa332c\u0026quot;), data = augment(fit_arimax)) + labs( title = \u0026quot;ARIMAX model\u0026quot;, subtitle = paste0(\u0026quot;RMSE = \u0026quot;, rmse_arimax)) + scale_color_identity() grid.arrange(tslm_plot, arimax_plot, nrow = 2)  \nI\u0026rsquo;m a bit surprised that the fitted TSLM model performed in parity with the ARIMAX model. I think this might vary a lot depending on the data you end up with. Both models do a decent job on the training data. But they both do a poor job at forecasting the sharp drop in movement that occurs at the end of the time series. I should point out that the ARIMAX model is forecasting the AR(1)-process recursively here. Consequently, there is a mean reversion and it looses its effect over time.\nSince I\u0026rsquo;m interested in the 1-step-ahead forecast, I wrote the loop below to capture what that looks like on the validation data. The point here is to capture the direct 1-step-ahead forecast instead of a recursive forecast, and thereby utilize the full potential of the ARIMAX-model.\ntsibble_se_loop \u0026lt;- data.frame() direct_pred \u0026lt;- data.frame() index \u0026lt;- 15:nrow(tsibble_se) val_weeks = 1 for(i in index) { tsibble_se_loop \u0026lt;- df_se %\u0026gt;% mutate(index = seq_along(1:nrow(.)), hits_nar_lag = lag(hits_nar), hits_beh_lag = lag(hits_beh)) %\u0026gt;% as_tsibble(., index = index) %\u0026gt;% filter(index \u0026lt; index[i]) %\u0026gt;% mutate(type = if_else(index \u0026gt; max(index) - val_weeks, \u0026quot;validation\u0026quot;, \u0026quot;training\u0026quot;)) # Save the training data tibble_train_loop \u0026lt;- tsibble_se_loop %\u0026gt;% filter(type == \u0026quot;training\u0026quot;) # Save the validation data tibble_val_loop \u0026lt;- tsibble_se_loop %\u0026gt;% filter(type == \u0026quot;validation\u0026quot;) fit_arimax \u0026lt;- tibble_train_loop %\u0026gt;% model(ARIMA(retail ~ hits_nar_lag + hits_beh_lag + pdq(1, 1, 0))) fc_arimax \u0026lt;- fit_arimax %\u0026gt;% forecast(new_data = tibble_val_loop) fc_arimax \u0026lt;- as_tibble(fc_arimax) %\u0026gt;% select(index, .mean) direct_pred \u0026lt;- rbind(direct_pred, fc_arimax) } direct_pred \u0026lt;- as_tsibble(direct_pred, index = index) arimax_direct_plot \u0026lt;- tsibble_se %\u0026gt;% mutate(color = if_else(type == \u0026quot;training\u0026quot;, \u0026quot;#7e828c\u0026quot;, \u0026quot;#7e828c\u0026quot;)) %\u0026gt;% ggplot(aes(x = index, y = retail)) + geom_line() + geom_point(aes(x = index, y = .mean), color = \u0026quot;#aa332c\u0026quot;, fill = \u0026quot;#aa332c\u0026quot;, size = 2, alpha = .8, data = direct_pred) + autolayer(direct_pred, alpha = 0.8, color = \u0026quot;#aa332c\u0026quot;) + labs( title = \u0026quot;ARIMAX model\u0026quot;, subtitle = \u0026quot;with 1-step-ahead direct forecast\u0026quot;) + scale_color_identity() + theme( axis.title.x = element_blank() ) grid.arrange(tslm_plot, arimax_plot, arimax_direct_plot, nrow = 3)  \nConclusion The direct 1-step-ahead ARIMAX forecast does a little better than both the TSLM and the ARIMAX recursive forecast at self-correcting for the last drop off in movement. But still, it essentially fails to predict that last drop 1 week ahead. Leaving that aside, the level of predictive power in the model is surprising considering its simplicity. What I take away from this experiment is that with a bit of fine-tuning and perhaps added complexity it is possible to predict a leading economic indicator with high-frequency narrative data.\nThe fact that the virus search variable did better with 1 lag as predicted while the train ticket search variable did worse is interesting. It might indicate that people don\u0026rsquo;t plan ahead when traveling by train or that train travel has an added element of spontaneity in times of high uncertainty when there is already a pent up demand for travel.\nFor future experiments it would be interesting to explore a machine learning technique with a larger and more varied narrative data set. The statistical rigor offered by conventional techniques are not as relevant in a forecasting model as they are when exploring a casual relationship.\nReferences Andersson, Michael K. and Reijer, Ard H.J. (2015), “Nowcasting”, Sveriges Riksbank Economic Review, 2015:1, Sveriges Riksbank, pp. 75-89.\nShiller, J.R. (2017) Narrative Economics. NBER Working Paper, No. 23075.\n","date":"2021-01-31T00:00:00Z","image":"https://dfornis.github.io/dfornis/p/nowcasting-a-leading-economic-indicator-in-times-of-uncertainty-with-r/nicolas-perondi--Ho_obgLFs4-unsplash_hudba9150e16cdf60946dabab36cfea944_1220069_120x120_fill_q75_box_smart1.jpg","permalink":"https://dfornis.github.io/dfornis/p/nowcasting-a-leading-economic-indicator-in-times-of-uncertainty-with-r/","title":"Nowcasting a leading economic indicator in times of uncertainty - with R"}]